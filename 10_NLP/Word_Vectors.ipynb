{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import zipfile\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import itertools, collections, random\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download()\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read all data from 3 files \n",
    "train = pd.read_csv( \"labeledTrainData.tsv/labeledTrainData.tsv\", header=0, \n",
    "                        delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv/unlabeledTrainData.tsv\", header=0, \n",
    "                        delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word2Vec expects single sentences, each one as a list of words. \n",
    "\n",
    "def review_to_wordlist(raw_review, remove_stopwords=False):\n",
    "    '''Returns a list of words'''\n",
    "    review_text = BeautifulSoup(raw_review,'lxml').get_text() \n",
    "    letters_only=re.sub('[^a-zA-Z]',\" \",review_text)\n",
    "    words = letters_only.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\")) \n",
    "        words =  [w for w in words if w not in stops]\n",
    "    return words\n",
    "\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append( review_to_wordlist( raw_sentence,remove_stopwords ))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 25s, sys: 3.44 s, total: 4min 28s\n",
      "Wall time: 4min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentences = []\n",
    "\n",
    "for review in train.review:\n",
    "    sentences += review_to_sentences(review,tokenizer)\n",
    "    \n",
    "for review in unlabeled_train.review:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews : 75000\n",
      "Total number of sentences : 795538\n"
     ]
    }
   ],
   "source": [
    "# numbe of reviews\n",
    "print(\"Number of reviews :\",len(train)+len(unlabeled_train))\n",
    "print(\"Total number of sentences :\",len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorazing words 1: Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 3s, sys: 624 ms, total: 5min 4s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 3       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "model = word2vec.Word2Vec(sentences=sentences,workers=num_workers,size=num_features,min_count=min_word_count,\n",
    "                          window=context,sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.init_sims(replace=True)\n",
    "#Saving the model\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('socialite', 0.7162072062492371),\n",
       " ('prostitute', 0.7108803391456604),\n",
       " ('waitress', 0.6776148080825806),\n",
       " ('housewife', 0.6685196161270142),\n",
       " ('wealthy', 0.6460838913917542),\n",
       " ('nurse', 0.6441401243209839),\n",
       " ('widow', 0.6410201787948608),\n",
       " ('gigolo', 0.6342802047729492),\n",
       " ('heiress', 0.631268322467804),\n",
       " ('lawyer', 0.6177793145179749)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exploring model results\n",
    "# businessman - man + woman = ?\n",
    "model.most_similar(positive=['businessman','woman'],negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16490, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 16490 words, every word is 300 dim vector\n",
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing words 2: Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17798082 words\n"
     ]
    }
   ],
   "source": [
    "# list of words\n",
    "words = list(itertools.chain(*sentences))\n",
    "print(\"There are\",len(words),\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 559142], ('the', 1014971), ('and', 494087), ('a', 491473), ('of', 439756)]\n",
      "Sample data [15, 30, 10, 520, 165, 183, 31, 1, 558, 15]\n"
     ]
    }
   ],
   "source": [
    "# data               ->  [index, ...]\n",
    "# count              ->  [[word,count],[...]]\n",
    "# dictionary         ->  {word:index}\n",
    "# erverse_dictionary ->  {index:word}\n",
    "\n",
    "#vocabulary_size = 50000\n",
    "min_word_count = 40   # Minimum word count                        \n",
    "\n",
    "def build_dataset(words):\n",
    "    count = [['UNK', -1]]\n",
    "    #count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    count.extend([tup for tup in collections.Counter(words).most_common() if tup[1]>=min_word_count])\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "del words  # to reduce memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, skip_window):\n",
    "    global data_index\n",
    "    \n",
    "    num_skips = 2 * skip_window\n",
    "    assert batch_size % num_skips == 0\n",
    "    batch  = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    \n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [ skip_window ]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "# print('data:', [reverse_dictionary[di] for di in data[:40]])\n",
    "\n",
    "\n",
    "# data_index = 0\n",
    "# batch, labels = generate_batch(batch_size=40,  skip_window=10)\n",
    "# print('\\nwith  skip_window = %d:' % (skip_window))\n",
    "# print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "# print('    labels:', [reverse_dictionary[li] for li in labels.reshape(40)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index     = 0\n",
    "batch_size     = 200\n",
    "embedding_size = 300     # Dimension of the embedding vector (number of neurons).\n",
    "skip_window    = 10      # How many words to consider left and right.\n",
    "num_skips      = 20      # How many times to reuse an input to generate a label.\n",
    "vocabulary_size = len(dictionary)\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "\n",
    "valid_size     = 4  # Random set of words to evaluate similarity on.\n",
    "valid_window   = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled    = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels  = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "    # hidden weights\n",
    "    embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    softmax_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                         stddev=1.0 / np.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                               labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "\n",
    "    # Optimizer.\n",
    "    # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "    # This is because the embeddings are defined as a variable quantity and the\n",
    "    # optimizer's `minimize` method will by default modify all variable quantities \n",
    "    # that contribute to the tensor it is passed.\n",
    "    # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # Compute the similarity between minibatch examples and all embeddings.\n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "        normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.755709\n",
      "Nearest to too: reeks, merit, romanticized, screwed, tourneur, wizard, lurks, kiss,\n",
      "Nearest to are: highlight, crossword, becomes, fisher, fable, neurotic, conflicted, art,\n",
      "Nearest to with: appropriate, universal, intervals, rumor, vanishing, survivors, occasions, warsaw,\n",
      "Nearest to also: vulnerability, implication, jury, learn, judy, gestures, compensation, sharif,\n",
      "Average loss at step 10000: 4.480114\n",
      "Nearest to too: merit, reeks, jafar, bogarde, wizard, rockets, screwed, romanticized,\n",
      "Nearest to are: kinda, keys, turned, waiter, art, bendix, of, highlight,\n",
      "Nearest to with: and, UNK, in, as, a, to, archie, of,\n",
      "Nearest to also: vulnerability, implication, jury, compensated, option, initially, blend, flushed,\n",
      "Average loss at step 20000: 4.239134\n",
      "Nearest to too: merit, screwed, bogarde, hundred, romanticized, controls, reeks, intelligence,\n",
      "Nearest to are: they, were, some, all, and, with, romy, desi,\n",
      "Nearest to with: on, of, some, as, from, to, are, and,\n",
      "Nearest to also: resonance, vulnerability, implication, option, compensated, compensation, forget, archetype,\n",
      "Average loss at step 30000: 4.190991\n",
      "Nearest to too: merit, screwed, damage, controls, restrained, versus, reeks, romanticized,\n",
      "Nearest to are: some, there, is, were, not, no, of, lecherous,\n",
      "Nearest to with: in, UNK, corpses, stockwell, on, and, up, maurice,\n",
      "Nearest to also: compensated, is, vulnerability, resonance, adamson, youngster, rocco, bravery,\n",
      "Average loss at step 40000: 4.166897\n",
      "Nearest to too: good, really, merit, jafar, assessment, aggressive, edinburgh, screwed,\n",
      "Nearest to are: some, their, were, characters, aging, two, other, of,\n",
      "Nearest to with: by, UNK, like, and, the, to, butt, very,\n",
      "Nearest to also: compensated, youngster, vulnerability, rebels, homeland, murders, policeman, compensation,\n",
      "Average loss at step 50000: 4.148707\n",
      "Nearest to too: very, good, really, cw, merit, wu, mama, restrained,\n",
      "Nearest to are: were, characters, other, their, there, or, scenes, tiresome,\n",
      "Nearest to with: by, and, his, of, on, UNK, has, in,\n",
      "Nearest to also: compensated, murders, homeland, resonance, gem, compensation, story, as,\n",
      "Average loss at step 60000: 4.139223\n",
      "Nearest to too: really, good, very, merit, so, plot, just, cw,\n",
      "Nearest to are: there, were, they, characters, some, their, these, scenes,\n",
      "Nearest to with: and, from, the, on, of, by, pushing, UNK,\n",
      "Nearest to also: murders, footloose, homeland, very, atwill, capsule, music, khanna,\n",
      "Average loss at step 70000: 4.130613\n",
      "Nearest to too: very, it, so, really, good, cw, plot, but,\n",
      "Nearest to are: characters, were, they, their, some, people, other, scenes,\n",
      "Nearest to with: in, a, up, who, and, by, UNK, she,\n",
      "Nearest to also: footloose, very, a, cast, murders, and, romeo, music,\n",
      "Average loss at step 80000: 4.122117\n",
      "Nearest to too: so, good, very, cookbook, anders, farrell, proper, pretty,\n",
      "Nearest to are: were, these, characters, other, their, two, people, scenes,\n",
      "Nearest to with: a, can, lerner, UNK, that, nurses, mumbles, the,\n",
      "Nearest to also: kahn, cast, adequate, very, supporting, murders, underwater, johansson,\n",
      "Average loss at step 90000: 4.117200\n",
      "Nearest to too: it, but, much, bad, plot, acting, pretty, so,\n",
      "Nearest to are: were, people, characters, re, these, them, two, some,\n",
      "Nearest to with: of, mc, and, the, pose, a, grenade, dismiss,\n",
      "Nearest to also: very, good, great, and, though, johansson, cast, too,\n",
      "Average loss at step 100000: 4.105681\n",
      "Nearest to too: so, much, lumley, acting, nothing, conclude, collette, way,\n",
      "Nearest to are: were, them, some, these, characters, other, there, scenes,\n",
      "Nearest to with: burnett, and, is, friendly, miracles, reprises, a, levels,\n",
      "Nearest to also: great, cast, some, funny, preview, romantic, as, rid,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  average_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batch_data, batch_labels = generate_batch(batch_size, skip_window)\n",
    "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 10000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss = average_loss / 10000\n",
    "      # The average loss is an estimate of the loss over the last 10000 batches.\n",
    "      print('Average loss at step %d: %f' % (step, average_loss))\n",
    "      average_loss = 0\n",
    "      # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "      sim = similarity.eval()\n",
    "      for i in range(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8 # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "        log = 'Nearest to %s:' % valid_word\n",
    "        for k in range(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log = '%s %s,' % (log, close_word)\n",
    "        print(log)\n",
    "  final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_context(positive=[],negative=[]):\n",
    "    vec = np.zeros(embedding_size)\n",
    "    for word in positive:\n",
    "        vec = vec + final_embeddings[dictionary[word],:]\n",
    "        vec=vec/ np.sqrt(np.sum(vec**2))\n",
    "    for word in negative:\n",
    "        vec = vec - final_embeddings[dictionary[word],:]\n",
    "        vec=vec/ np.sqrt(np.sum(vec**2))\n",
    "    sim = np.dot(vec, final_embeddings.T)\n",
    "    nearest = (-sim).argsort()[1:9]\n",
    "    words = []\n",
    "    for k in range(8):\n",
    "          words.append(reverse_dictionary[nearest[k]])\n",
    "    return list(zip(words,sim[nearest]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.4365889754428649),\n",
       " ('buddhist', 0.22472288890400319),\n",
       " ('solving', 0.21947928440538284),\n",
       " ('biography', 0.20472893888160873),\n",
       " ('kathryn', 0.20261553766788787),\n",
       " ('backseat', 0.2010753473947694),\n",
       " ('heist', 0.19771546896371606),\n",
       " ('conan', 0.19381510351280007)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_context(positive=['businessman','woman'],negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vec2Word(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(Vec2Word, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, batch):\n",
    "        embeds = self.embeddings(inputs).view((batch, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = Vec2Word(len(dictionary), num_features, context)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "num_steps=len(dictionary)//batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.49053001]\n",
      "[ 7.77449656]\n",
      "[ 7.3751483]\n",
      "[ 7.08530188]\n",
      "[ 6.99869204]\n",
      "[ 7.07109833]\n",
      "[ 6.9630127]\n",
      "[ 6.74427176]\n",
      "[ 7.1649332]\n",
      "[ 6.72411203]\n",
      "[ 6.82953978]\n",
      "[ 6.88935375]\n",
      "[ 6.80248642]\n",
      "[ 7.01510191]\n",
      "[ 6.78813505]\n",
      "[ 7.02150917]\n",
      "[ 6.95554161]\n",
      "[ 6.95084715]\n",
      "[ 6.86200857]\n",
      "[ 6.91851234]\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.1)\n",
    "for epoch in range(20):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    for _ in range(num_steps):\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in variables)\n",
    "        batch, labels = generate_batch(batch_size,context)\n",
    "        \n",
    "        batch_var  = Variable(torch.from_numpy(batch).type(torch.LongTensor))\n",
    "        labels_var = Variable(torch.from_numpy(labels).type(torch.LongTensor))\n",
    "        \n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(batch_var,batch_size)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a variable)\n",
    "        loss = loss_function(log_probs, labels_var.view(-1))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.data\n",
    "        #print(loss.data)\n",
    "    print(total_loss.numpy()/num_steps)\n",
    "    losses.append(total_loss)\n",
    "#print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.embeddings(Variable(torch.LongTensor([dictionary['businessman']]))) - \\\n",
    "         model.embeddings(Variable(torch.LongTensor([dictionary['man']]))) + \\\n",
    "         model.embeddings(Variable(torch.LongTensor([dictionary['woman']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine = []\n",
    "for i in range(model.embeddings.num_embeddings):\n",
    "    embed = model.embeddings(Variable(torch.LongTensor([i]))).data.numpy()\n",
    "    cosine.append(cosine_similarity(embed, result.data.numpy())[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indx = np.array(cosine).argmax()\n",
    "reverse_dictionary[indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1957472"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine.pop(indx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
