{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "LSTM\n",
    "=============\n",
    "\n",
    "------------\n",
    "\n",
    "The goal here is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "# text is a long string of charachters\n",
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        #data = tf.compat.as_str(f.read(name))\n",
    "        data = f.read(name).decode('utf-8')\n",
    "    return data\n",
    "\n",
    "text = read_data('text8.zip')\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 26 0\n",
      "a z  \n",
      "Unexpected character: ï\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Utility functions to map characters to vocabulary IDs and back.\n",
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter    = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' ')) #, char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))\n",
    "print(char2id('ï'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size     = 64\n",
    "num_unrollings = 10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "batch = train_batches.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['y defined b', ' was open f', 'united stat', 'amed after ', 'e reference', 'd in one ni', 'he turbulen', 'to nine zer', 'age as the ', 'ro s belize', 'filters cra', 'aisle ratio', ' with the t', 'many aspect', 'f the legio', 'ut the shuf', 'eight zero ', 'sm for the ', 'e two eprin', 'rse has alw', 'ded the con', ' crystal st', 't is n bits', ' three to o', 'itory or vi', 'ed to accom', ' variety of', ' an indian ', 'le parties ', 'are to tran', 'ilar inner ', 'e down the ', ' to print d', 'ich plays t', ' remaining ', ' s aircraft', 'y ways inte', 'in the eu p', ' decay fiss', 'er to entir', 'either to r', 'he river in', 'frac two x ', 'pe a and d ', 'ly was unif', 'rger amongs', 'r the unite', 'pagation of', 'reported be', 's been seen', 'f the econo', 'ng on also ', ' the compan', 'hikuni the ', 'eaving the ', 'ity francis', 'hooting the', 'ero zero a ', 'ng and the ', 'ociation fo', 'ommonly ref', 'e four one ', 'er faiths o', 'pls works m']\n",
      "['by what it ', 'for per n s', 'tes natural', ' the french', 'es one one ', 'ine four se', 'nce of the ', 'ro mph stru', ' city too b', 'ean creole ', 'ate fender ', 'o in econom', 'three being', 'ts of moder', 'on of merit', 'ffle cut an', ' s has been', ' fine folk ', 'nt liddell ', 'ways existe', 'ncept and t', 'tructure of', 's long that', 'one seven s', 'isual stimu', 'mmodate cha', 'f sexual pe', ' tribe that', ' and have s', 'nsfer e mai', ' focus as w', ' first elo ', 'documents f', 'the role of', ' three two ', 't in the po', 'ermediate b', 'proponents ', 'sion and fu', 'rely differ', 'resign or s', 'ndus which ', ' a zero cos', ' found pred', 'fied in one', 'st others i', 'ed states g', 'f linux on ', 'eing one fi', 'n by very f', 'omy in sett', ' known as b', 'ny was orde', ' first post', ' once domin', 's bacon jac', 'e cello s b', ' translated', ' support fo', 'or childbir', 'ferred to a', ' st state i', 'or deities ', 'mpls works ']\n",
      "['gi']\n",
      "['in']\n"
     ]
    }
   ],
   "source": [
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "    if s >= r:\n",
    "        return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACDCAMAAACqXRB0AAAAgVBMVEX///8AAAD4+Pjt7e37+/sy\nMjL39/fz8/NoaGg1NTXw8PDm5uanp6cNDQ0aGhrp6ekoKCiFhYVPT0+1tbVkZGRbW1tvb29CQkJ1\ndXXFxcWqqqqVlZV9fX2ioqLh4eHMzMzV1dVUVFSNjY29vb09PT1JSUmysrKZmZkiIiIWFhYqKirW\nLILXAAAZCElEQVR4nO1dCXuiPBAmECDct4ByRMWj/v8f+OUCCWpF2+7ab32fZzcthZAMk8kkeTNR\nlDfeeOMB7P92AR5A2/ztEoxQ1u2zj0apotgagdonCk2eyIk+ptmKotIUnq8HTvlk2WwHO9IFrL+O\n1LcfsfdkxVa6odg6IChEogQ0qR/OSfPocyS7VH6+BsB75hvyHCflqEPzqay+H6a108FzVgKzj2Vv\nQO5AmugkUZqwCGY9bUu/BRsQB0QmagsK56zpEINcnVseWaRaBrD8dyNHc7P6YZSgKN3nHkW5QZMa\nWDR1eVJ2xqyHYSc3/gxELMUfkmYnpPXMBEyT8a9mGNqTO2oPz83sZ7F6whhwaD4XE5E20THD4kk3\n01QZsoW1Q8AfXMVwfL0AkiQ/A0wl5cEguyjxRzQ3s5+E6SBQOvD+jaTEjeM0zUgNEyEmDCyiUnXG\nk1R6yChdQ2ndKxZiInQMfJ51JsnFyP0mqed1gBOhr8Bm78q6rubxbFv1c1BT3fN0XVImqI0wXDWQ\nT7s4MGr8EeAGwqF9nXYsAdBIt4qlvHbrdVh0XXxpciZCXwGu4eZRaimBZ+3WnTfLvZoIPQM6ebls\nw5D/XK/8vTAa3wrkDmilj9D3sEHuodUJbMbeJTrxGhCHJVCKSCTij9Al1YXrmnwSq/GueCAToV+Y\n9JZpwh6cMMniagcIJw1UFrodenvF8XiP0Ii/rID8Ef4SMK+QdhiuXNV0RJWtlsw/zCzeeIlz5mDd\n1nyW9H9c+6bi7AzaUas7mjve9k8aAUGjtzQRbzCtiUkvdx69vwAu7etZGSNJyM0yXypyjtmBJkFf\nMWLSNdbPEI/qWLGLNcDPCOm7UYMVTar8U7vOq+1KQldzizcR2wJNWtOkTPsbWrCh1TSoEUoUmjdc\nL8TfzIw1Io+3JS4jZ2LSYR7FDjXptI1ErDNtwdhAw6gAg9BVxLLyQ5Zgdm1FK1ZyjypdZPyTuuBJ\nR+17gZiG2Xk1CF1tRhBXV6zakfAkVCZsiISmEy2tiJ8oEo6NJX5SYyo2SNqP7/avUE2CQMc0EabN\nFe64fcTsd9vb0rsDj1wmnSkZ6hobS+4VzbPQFUhz0tJ6lCMCmJaYKothbcSra/Ds+PY7oTJV2lhe\nuOhFchjbdNHFImKuiVS5am+zrKL1RT6XAnEWP0qW9CNbFVknROwRLBLNI858WxFbBKzD+M2yTRcW\nhM0rENQIINoCmZdODIWx26KThaTueCx0BsmmQ0QqprKxdoJALExT9BI23T5RlYSJ55x9KXUEcWkP\nMCkxs7JKGTZGuFHOvRLMmMREwhBYFbUWEUhrctHoEkWNLE2yYLLQNZ85+YnFLxruyaESZl46GUls\nd0bgu7IL9LnQlYIIfc96Z3ULml79X8J7EY7BHZOuwDiv1z6TuWFVig2oE4P7cQviAz10Hu+ZHhNI\nAio9DQ9dNDbpAhPvZevFbhTv+hmExZoWiBoWnkvDTXqJBIy7Qm/CdSRGb4fe1hn5/DmFHwTvWdRd\npdyR+n5RcyXRwFZZ+lRz7FCM0DesL1ZW5262BfzmfYXJP2p0TH+pfKLp5BOiLN33IlFz7m24SZ8L\nMYG03dkc9PV3hK40K/oYBf+CtOze3x+RwmJVeFS3SLfVzp3i0ECjZLwW0c1Zu2U4HQ0RRXUlmUA0\nbemjb0I/0eSP64WJJDW9ELpSXR9DqdZG/BS9gHWpQdcx62L6kxmPz7CIM5/XQgtvOWCL9fRK668m\nUyGfzYs14EI6WZxJngfpa3O5Z1VumA7TEyME0/r7iq60Xq7zgjrXZkeuA0KtAUKlkhvLAka+ubiG\nZ09ckecvWwppPfK77LmrJUbriQZZvIRFt8t507AjwHUFK6sv/HZ9rRqLKvzSGk1roeorz0vAViFa\n8VKfN9H/gljEaH0u/DW3V831S0V/BHvQfZ942o9+4t54WMNeB+bdNS/41e5K+07x2K+yRvfGG2+8\n8cYbb7wCXmGm9B7+/tD/e0EHR6qhqiqkiUFHSobxuD+sGuIpwxhmkxXV1GZPTUxg2qP1DnX9tVHD\nj6B5YHw+QUJGnsbasiw94kmlaDpJHiSMqexZ3Vb2OsuDwdhZ/pPSqkguow9vhy9Hc7U/nl5NsUM6\nmZsgYNWNskUgrElOtZ/Xj05xJBvg1UtirA6+NZBckviBtTWpTbSFTB8oX4XYNUDNnp56E1O7NWBz\nMYJWNxACPsVGbl4968U+DpQBxbTmT8S28lzNhLVmWK/CYRzwrOEcFjG4tGHG6A6U6jIDqXwXukJk\nDMB8OpabjmtBiiI33pX3WlNdhjO32+N8mNFERimW60rGpys7kYwkpTkqecE10clCv2C9UNQg0pyZ\nwpKFbp+sxhm3EjyfEPkn4HRHfTohdGVhmqCOrc7quu5892qg1RE7YOp7yq6T+KOurutlp1+j8cpC\nH1gv6cjUIVB0uicxCG5CFjoGna5/nA2Vovmzqb9/ADCr6yn/xk1HGERYgDzzQIpW5xuRz/UwoCTG\nOmW0ujF/dL/TghBsd9f6Q1notbAkZshu5aRGC+xsM79u16cmURY66SEaMxsRpU1r/sLYz8M52oI6\nj4drgbskcDn6skZgqcKV9H0GspEGgKMdG8qus48N/1NLanrCRMVyLWfd7Dl/RghKDyNe0MSklxl9\nIKA9hcE700CyMtDeLqCcXZ3aNOFXIasSJRvBBV/iMizrlebSNUYGIt7E5ytZ2KMLnBNanXWm1TlF\nRBNcceEZFlbYSLAEBRn5KGwtXzxWjBlw3IGemPQDU9uafgheugl9o9T1ITdly/IJfZZwSpEdkoKp\nObHjvBwjBuCLgFH+FJiffa5rBFJug6J+mwyr25hWdyB+Ikk2wl2EvUx6It5IbNChSFcs4ZeCfl9R\nJpn0UhFsIdsb2WeaxeIsdIXlc8hYwq9y6qhPPJa+HC+m6ZTOU7cu8uKhvhEYQdQW0T5TjbmBbRDa\nUs1Ep4FWd6x5ws1P2bEWUbZqTGquVqqL/Fjm3Eo2PeAevtKGjBaWUWPCvHRqKJJDlAEksy3GQmeQ\nbHpC9Sghnn8ZCyUxrekDfxUOyGwdG3uvGdrfNU1fUS7jnhN8HWvbfNCGEQFuamHM2J80YbKzY0ap\naoHeeOSHVaoYZ2KbgCR0O2efr2HD9UPNWkXgkb6VdAn2rrSLfOJ2fi50h1hBGHuNKAd9QfhSoyPH\n26QFpdV9rglmiMoVJ6nBdSrae9m7JYgTR5H4vW3XKfvzIov9MqI0jc1USrL3svUyjKMjm2o5BGzf\nRcJMup9Fa8hsH6x5z05nGu4IHcY5Rv62LwetJVgprwR3F8km/TqaYifcRxssiZCoZvYbvZSqGCfU\nBrdUBqv1SonWi8ueULkYkZaxrqeY/7xhkwvtmkp3mxE/hZOFkiWDS1vX50JXYLEraGYNL8fL7AU4\ngxbf9PbNvRmYvpoaccoF37Tg5qTfhzJsR9mEOOof4f63v3Tk/BeTyQJ1WLNXd5uI+p0iT/JvC+yl\nPKK8EPpe3l4mCrsJea1g/Epueg/NK7O5U43QWmx83jDwjZ3Wi0UqswptD0/yvy0EogCZLNJtWGbj\n+x03990JbfGqR7hYZOw2/BJbAS6wR/MpWdBd9i7cSr/qiQVoWslH8t8X00HoZrLbqCZWZs5+O1EO\nNX6pbvQZVAfF/egVK0o/vfc1EL2icXkIaq4j66zI+19Qn5eaYXwOKm5eaXT3xhtvvPHGG2+88RB+\nA62O47X4AF/Bdjem1amcVqc+MXet0scYsY4n/WXzWSIiNG3Js9X0l5wKeAKUVqd+nVZHg1DRx3RT\nqUfEOkWxdetJShxE1oRh1O+A/+2QaHUJpdU1nFY3pynLQ9mkAH69VZTmQB4/26w6fDJAoLItpnPo\n6KWW7J7GDVrdPDFV8rpnJMKcaR94dFXz89mS2stCZqFHxrg1J/q7YF6j1SnzaHXTZQwYC8Uspbh+\n5fwAgcpBMiemdZpwKmH8ClHTGGwnmDlnBYdQCAI3aHVy/+VAxbwaDU+OnqD5ALMfImkzewQSzZlp\nYApJ6A6IA0eeZY+eNVXfDLPQ9eM0fCEc4XyV0eos6xatztCTC1odqad+tLadnl2Ruiz0EnBTBXdj\nI6HGXtEdfdkQ3QKShF6DHXm35NGWr8Fo1E4nbGBPnhS/RauzHqXVkZrv7MbzSRu44s7LQo8Eo3xq\n0kFlBjfs+vRDykJHILadUDIoztNhP78TPNqSadGwUHi46rgjnGl1tUFpdeNF99u0OgHtGFDCsxYy\nTR/ewAJu2VLArZFJH5uEktOGeActFR3ay9E2Dc7US0cZmhbpG1RLMijB9UiDfxgl421qPinaXVpd\nrHxKq4s4rS6Sl58DztEyad5VH+kGSsQ6QTEIAWY/yCadRZPi4b3MD6ll1Ll/JhjwSJI+p9bxIjp0\nlwHJdey8ai8hdM7bxMTrgLsRB+MTWp2o+D1a3ajt92QY1RreYHBi3WFErHMAd1rUePwGNadk/j5A\noDS2gaZ1Frpg6qUs4W92adUwz7Uvy2toOhKB/yJGe+uvShFIr9LqimJKqztJtDpFrTIWOG3fMnac\nUUGX2FjZoko2HXNXX9lysnyLMsqjY/RRM/ftukSxj6S1aNWaUmkkm17TqkVU8k0RC0PkPOB//hyY\nDmHST5n1V2h1KufTDbQ6xQkPBrUmJehayrgrKsWs/ckqnyR0zWdRdRqL+RebsMF+K7x0TIl/QRAv\nZH/vjtBdgIm9Ix1E6W9NEfIHv0RHaoYxrk+xNotW5xy8m7Q6INHq6B3K8mSSr7SJrVPLQmQOwct6\nyN6LCwoHR3ybVwta0mwWvVIASvxjQb0Gal1wV+h2WOD45JCCbwZu0grg2aL5QZhFl9L9h3Nodd1t\nWl1RjBPSYS1FRSPidRe7QqXmebonVBa6kuS6jjD7cXEy2FsEWW5FzQMLEAhdQa1T7gpdadAusik7\nrFHUHb81e5XTGVS+1cRbPkOrQ9dpdTSenekf+od4OFdvi+W5kQmbUTGGwe4ipxaGdrFwePPmZFRS\nT3op9GJisdl7l8BUGs6NGnTkRaBd0N5u4z6tTrUOzZQxqgGM5Dfc1rqtj/fWJOcq30uK3Lph7k6Y\netcmG5yPBIsIgZE/Z4PrH8SztLobY8VNcTFy3xfzF5latLooTiWraUssjTunzE4Vca+nuRnP8BdA\nptW9ghM2C+ruBcMyzIWax0X4u2h1DPDlgjI8AqN90+reeOONN9544403nsAv8aG+XEw4d2X+D4BO\ng6kqpJNHBklErLvHqzjEtaGZDVehaT87AjFsabph88WVExh33ryV+QdR4ocfyTJFaXVK5dNKRsnT\nlA1JZp7LeYbJCH07Q9nQtJ+EMzLLenIEu9It6aQ+Nf7aPFvkoe9Z8ZpIpgQX0ZXuYe83NLhdCJCr\nBPWJJgouAJpDspCWDmBdAIsS+tqNNyL0JfEDfA05VF40OZKz8b7C/DBDq15/BzfaTuWm23SPzkWJ\n0y1Uiy0GwZivCUXT2d6rgPK6Hw1bxdJgHF2Nn982Dxeh8qYkyq8wy+iZSN8y1eJM9/w+bIkTgGkC\nEZM29lg9m3ksSlWXPrFhCZ2ux6aJBeGYCTkwgXJxynJ5uWF7dkQ4uwYbe1KSCedtJgVuInT7svXY\njnxeJz/d6Pw7EotA3NylR55IyyJaQAl9V94+EXoAhIaj8YzpHkT2k6HywrCZSGUats0ojvpxYumv\n0+o2eghCXW5JlJR3zrBBR/00p/uRhV7qPpo0oLbTjzk+34EsijOt37CEHjKhl3rKEqkTjbpjtu2O\nV7qgidATsZxuSEfhFixU3jyrNw2Vp+90a+xvTM/7hfkJ26EcKlKi1Z3Nn+GsAZaD3aSFbWi7XupO\nmOMmtK7o+tQmSUK39RZNaHdY35r2fiCLJV4Y+yBG50+j+UKYlI1n7MoVSdTduBFHa7MBXnkEl9o6\nEXoxmPRRyY0c7DQzfi5UXiiHyqPM+PFvakbMjebRvfN4uOjUZ1bdmN5vW/zZ4TvgI5WdfeT3mJaP\nldaj0jjnxf7QVudPZVDmW6MH59hy9do+MQLNEJIOrtn6ar9AUnqZATEYt4aBNlSDDNZrWINcdcd0\nA42Ydwwy7YM+BYdPyk/Q1BmhbzjO7rpJJ3UVnakcKk+x21GoPONmqLyxBiDpqKw959VBGnrobhcZ\n8Hi457NYOVkU7pK+/grfTlVJLpO9686h4hp9dP4rP4/LMPkp9XDdmy6Tn0ckzAWnWmreOB7JQBuq\ngRV0mDLkaTIuLA+4y0Y4i96pMfhZsd74rNjAu2bSWZH4joRJgKA2vhcqj3Q3aiwFU0RSe+vjIco5\nXz3YWxSExsnu31nu2EmKOlMk8oVFP2ZMhhXqchSfL6DMt1ZvaNJf5d39cNgZMRTsM7qcAOywj4l7\n3WY1GzSdSJvGUxKJhCHgrnEaopia9LWNPib0JSJomyEdKM5IlCyi1cUxeuplqLyUVcfoiwsp2/hM\nZrgUOvsg5m1a3WiRizEZMd1AJV5q6rQ4NQ+uS5opq4Z5QCCWO6DlNCii3JES82ZW5jgkXd1p9Chx\n3oI4azXi7bWpsgN7RAi9IX27xpJTfx44QvTj7RsWndE5KCsEMqk8sk1PxPc78GHkFmW499KJ7S3r\nCD0aKi8SzFwjyoQiIMnn59zMJiajKBf3F68fYS80J3C9zdAzN/oarfvzsgr2OZssWE0JdHeEXoMV\n1s1tbC0HfYj0NO2EFjAyHPY529pKXFoB0xLbDBzAPglJxIBkb+GIvC4BWUlVDkVETU4yV0MWeuAx\nDk/DOR+bvMmIEWWnLNt+aO8SG1mPh8rLPKwE1qH9WHLJSQKgvDocHZPLaHKXYIxvesjgqCc23Wo4\nDrfxYk6MYwS6RPTEtNe9K/QtaRoyyaxcrfrWDrMcJxZlarPzZzGzFf1OOY0bhz6wI+MVLffUnh9i\nK28K2mWl0/he+mQ7QhSUEWfKUj7YphxC5aF6B7nh7R0LrMwIlecUpBQwXSsO13T7NJkXKLpd1VyJ\nJneJgEV+/eTzNKjbbTRh0vvYcrSUd4SuFN1ybNKngNFuzc8eaPvTIcmHEi7TkV0xe3bxxhevWnQH\nuOjoaeUjk84xEbpS60e94O7pQrgT7Y6FyotpqDxm0vvq3I9PSOn3NFSeDYbX4osAhQZ94DKa3BTt\nvuTxYdcLWNxydHgoTNBOCHT3hM6euwxJNymjwjly3GPVfPEKW0q41DD9Se3L04CmlTfydZP3GH1Q\neK57jdLPidH/2/uh8pLJVBJ7LeUWKg1TjRvUsru0usADBYv4bORu9DmDpAU4HucF3cVFbLkrY46l\njq9tB5Ng+vvmwO+Krodsa0PcTrhgW1Ie+d7b7LqlFbRI9r22oVwuZxmf5oTKU63KPsTsWONbHMq9\nIMzeQuDtQt6HYzQ9ZHiKWibQqTS23Ixts5vi/lSHU6GES8DIr8cUJc7LNI7d4QFC3xIVU67epFyz\nQ+XBCq3Yx46e3qndVi+23bjRX2J/4n3sb5xO/CvhzFG3F0D5e6KzvPHGG2+88cYbApeDkd+yk+Bn\ncHVz1Ddjiy4GB4fX2uf2R1GjbLok/P1oPi6HVUb370p9Ff/8bnlBC5qg8V9sgPsHEf18rJ/6+hQb\nip+dkPjtYGc//SxM/XpbetFjKv4A6HK3OZu6Ca/cashhPhUVQjn+6ChulvR4f4Dsv4cSLFJdn3nu\nUpnp+tQ8r3R9HCgtKRaLBULjJYNVvxCOyeMjuhWM5wdc/H8hAh+ulgIsX92iM9JhiizyDlrrySy1\nIsZa0q+U0ZVr3we+ZR36OA8KDcTDFywSr3A2YLT8gV7sBOI/BTWmUhCd6VnvylRIPDocDr3QV4Bu\nfGCByIY7S7Z6U3biZMbM29tB7jsmPUVRrDLDmLPCEvK5TJ2ej9bbmOLpM9x/N1i0JsHJWeafOuyB\n5zPmA7m3HRbK+MYOk5ONBDWH82rsnm8kzqQkD2LiJzbkp54fVoP/0dz+Axhxx5QxHeDKGdOcM8fO\n+Fzs+s/DaXWCBEcUPWChzqg50fple3H6agn6tellH8uh/kc1nRmWPYjgvq39c7CQw5jhJXwPHqI5\nAXHjdll/Jz5i8v+ee9w202mNc+mXflnTv9GIdlTIdR/q0l1bIrQjusKn/QfAvfQM4FXWRqAehK42\nIwitrimRisbObGuwGpzAg75yI7EEyElXEQ+7s/CQy6lkiLGNBRMxSuqw6oX+IoFv/zCYYTHysNWx\nsrmzYm2Hsa1R4pKy9Ufudpmmw+r5ym+1FQ8iRZlYNrfZ3IqonYVtnHaO3VtyI/83XUbtSE3GVu+2\nMxheTdx1jKZR7aQed/RL0XWCl09N+oF/nIBzedWs67qVSuyOEPXFlpB/BdyPoPHKTH8J72z1EFtS\n1V2lHG44OqbWM7F8E67TLe1MYcypOtBmY9fFGtbs9wT8ksX8n4MGcHWLSijD/FhGh3s3EdGavpsx\nVS+9kWuoWlXCrLsaT+OH/4OosrnRlg7ZXZlDeqxuFYv5gWw8ii1jPmpKPPxYAf+XmL+9dMadzID0\n/aSxk0JRs/9/DT3r1wJGF37K/kciDLwxwr+9DH3Gf3NsjAWbPBRFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename='LSTM.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state  = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate  = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    update      = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state       = forget_gate * state + input_gate * tf.tanh(update)\n",
    "   \n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output  = saved_output \n",
    "  state   = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss   = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step   = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input        = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state  = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state  = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.293578 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.94\n",
      "================================================================================\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "================================================================================\n",
      "Validation set perplexity: 19.96\n",
      "Average loss at step 100: 2.586821 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.03\n",
      "Validation set perplexity: 10.68\n",
      "Average loss at step 200: 2.238401 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.48\n",
      "Validation set perplexity: 8.93\n",
      "Average loss at step 300: 2.099173 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.63\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 400: 1.999820 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.12\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 500: 1.932145 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.32\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 600: 1.913321 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 700: 1.861003 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 800: 1.828015 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 900: 1.842744 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 1000: 1.818730 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "================================================================================\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "================================================================================\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1100: 1.775819 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 1200: 1.759794 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1300: 1.738074 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1400: 1.758073 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 1500: 1.737746 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 1600: 1.750383 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1700: 1.711569 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 1800: 1.672540 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1900: 1.663874 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2000: 1.701110 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "================================================================================\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2100: 1.694584 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 2200: 1.681175 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2300: 1.651370 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2400: 1.669069 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 2500: 1.673213 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2600: 1.655731 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2700: 1.655169 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2800: 1.652272 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2900: 1.648890 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3000: 1.645574 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "================================================================================\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3100: 1.644598 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3200: 1.636121 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3300: 1.654388 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3400: 1.667970 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3500: 1.647004 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3600: 1.669176 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3700: 1.645796 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3800: 1.646496 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3900: 1.643062 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4000: 1.648139 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "================================================================================\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4100: 1.642290 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4200: 1.631826 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4300: 1.612432 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4400: 1.618015 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4500: 1.615416 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4600: 1.621165 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4700: 1.620822 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4800: 1.636343 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4900: 1.628041 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5000: 1.603053 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "================================================================================\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "================================================================================\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5100: 1.603374 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5200: 1.596547 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5300: 1.573903 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.17\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5400: 1.576281 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5500: 1.571792 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5600: 1.586488 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5700: 1.571152 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5800: 1.570884 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5900: 1.571998 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6000: 1.552962 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "================================================================================\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "================================================================================\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6100: 1.563301 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6200: 1.542274 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6300: 1.537213 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6400: 1.548075 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6500: 1.562586 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.07\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6600: 1.590660 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6700: 1.593247 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6800: 1.596223 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6900: 1.584793 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 7000: 1.581686 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
      "================================================================================\n",
      "Validation set perplexity: 4.21\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'z'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed = sample(random_distribution())\n",
    "sentence = characters(feed)[0]\n",
    "sentence"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
